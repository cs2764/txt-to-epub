import gradio as gr
import os
import re
import chardet # Import chardet
from ebooklib import epub
from tqdm import tqdm
import shutil
import numpy as np
from dataclasses import dataclass, field
from typing import List, Optional

# --- Smart Chapter Parser Data Structures ---

@dataclass
class Chapter:
    """æœ€ç»ˆè¾“å‡ºçš„ç« èŠ‚ç»“æ„"""
    title: str
    content: str

@dataclass
class PotentialChapter:
    """ç”¨äºå†…éƒ¨å¤„ç†çš„å€™é€‰ç« èŠ‚ç»“æ„"""
    title_text: str
    start_index: int
    end_index: int
    confidence_score: int
    pattern_type: str

    def __repr__(self):
        return f"'{self.title_text}' (Score: {self.confidence_score}, Pos: {self.start_index})"

# --- Smart Chapter Parser Class ---

class SmartChapterParser:
    """
    ä¸€ä¸ªæ™ºèƒ½ä¸­æ–‡ç« èŠ‚è§£æå™¨ï¼Œèƒ½å¤Ÿä»çº¯æ–‡æœ¬ä¸­è¯†åˆ«ç« èŠ‚å¹¶æå–å†…å®¹ã€‚
    """

    def __init__(self,
                 min_chapter_distance: int = 50,
                 merge_title_distance: int = 25):
        """
        åˆå§‹åŒ–è§£æå™¨ã€‚
        :param min_chapter_distance: ä¸¤ä¸ªç« èŠ‚æ ‡é¢˜ä¹‹é—´çš„æœ€å°å­—ç¬¦è·ç¦»ï¼Œç”¨äºè¿‡æ»¤ä¼ªç« èŠ‚ã€‚
        :param merge_title_distance: ä¸¤è¡Œæ–‡å­—è¢«è§†ä½œåŒä¸€æ ‡é¢˜çš„æœ€å¤§å­—ç¬¦è·ç¦»ã€‚
        """
        self.min_chapter_distance = min_chapter_distance
        self.merge_title_distance = merge_title_distance

        # å®šä¹‰æ¨¡å¼ï¼ŒæŒ‰ç½®ä¿¡åº¦ä»é«˜åˆ°ä½æ’åˆ—
        self.patterns = [
            # é«˜ç½®ä¿¡åº¦: ç¬¬Xç« /å›/èŠ‚/å·
            ("ç»“æ„åŒ–æ¨¡å¼", 100, re.compile(r"^\s*(ç¬¬|å·)\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶\d]+\s*[ç« å›èŠ‚å·].*$", re.MULTILINE)),
            # ä¸­é«˜ç½®ä¿¡åº¦: å…³é”®è¯
            ("å…³é”®è¯æ¨¡å¼", 80, re.compile(r"^\s*(åº|å‰è¨€|å¼•å­|æ¥”å­|åè®°|ç•ªå¤–|å°¾å£°|åºç« |åºå¹•)\s*$", re.MULTILINE)),
            # ã€æ›´æ–°ã€‘ä¸ºå¤„ç† (ä¸€)å°‘å¹´ / (1) / ï¼ˆ2ï¼‰å°‘å¹´ / ï¼ˆå…äº”ï¼‰èµŒèˆ¹å¬‰æˆ ç­‰æ ¼å¼ï¼ŒåŠ å…¥ä¸“ç”¨æ¨¡å¼å¹¶æé«˜å…¶ç½®ä¿¡åº¦
            ("å…¨åŠè§’æ‹¬å·æ¨¡å¼", 65, re.compile(r"^\s*[ï¼ˆ(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶å»¿å…åŒ\d]+\s*[)ï¼‰]\s*.*$", re.MULTILINE)),
            # ä¸­ç½®ä¿¡åº¦: æ™®é€šåºå·åˆ—è¡¨ï¼ˆåŒ…å«ç‰¹æ®Šä¸­æ–‡æ•°å­—ç®€å†™ï¼‰
            ("åºå·æ¨¡å¼", 60, re.compile(r"^\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡å»¿å…åŒ\d]+\s*[ã€.ï¼].*$", re.MULTILINE)),
            # ä½ç½®ä¿¡åº¦: å¯å‘å¼çŸ­æ ‡é¢˜ - åŠ å¼ºè¿‡æ»¤æ¡ä»¶
            ("å¯å‘å¼æ¨¡å¼", 30, re.compile(r"^\s*[^ã€‚\nï¼ï¼Ÿ]{1,15}\s*$", re.MULTILINE))
        ]
        
        # å®šä¹‰æ’é™¤æ¨¡å¼ï¼Œç”¨äºè¿‡æ»¤æ˜æ˜¾ä¸æ˜¯ç« èŠ‚çš„å†…å®¹
        self.exclusion_patterns = [
            # æ–‡ä»¶åå’ŒURL
            re.compile(r'.*\.(html?|htm|txt|doc|pdf|jpg|png|gif|css|js)$', re.IGNORECASE),
            # çº¯æ•°å­—æˆ–æ•°å­—+æ–‡ä»¶æ‰©å±•å
            re.compile(r'^\s*\d+(\.\w+)?\s*$'),
            # åŒ…å«URLç‰¹å¾
            re.compile(r'.*(http|www|\.com|\.cn|\.org).*', re.IGNORECASE),
            # åŒ…å«ä»£ç ç‰¹å¾
            re.compile(r'.*[<>{}[\]();=&%#].*'),
            # åŒ…å«è¿‡å¤šæ•°å­—çš„è¡Œï¼ˆå¦‚æ—¥æœŸã€IDç­‰ï¼‰
            re.compile(r'^\s*\d{4,}\s*$'),
            # HTMLæ ‡ç­¾
            re.compile(r'<[^>]+>'),
            # ç‰¹æ®Šç¬¦å·å¼€å¤´
            re.compile(r'^\s*[*+\-=_~`]+\s*$'),
        ]

    def _preprocess(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†ï¼Œè§„èŒƒåŒ–ç©ºç™½ç¬¦ã€‚"""
        text = text.replace('ã€€', ' ')
        return text

    def _scan_for_candidates(self, text: str) -> List[PotentialChapter]:
        """
        å¤šæ¨¡å¼æ‰«ææ–‡æœ¬ï¼Œç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å€™é€‰ç« èŠ‚åˆ—è¡¨ã€‚
        """
        candidates = []
        for pattern_type, score, regex in self.patterns:
            for match in regex.finditer(text):
                title_text = match.group(0).strip()
                
                # æ£€æŸ¥æ’é™¤æ¨¡å¼
                should_exclude = False
                for exclusion_pattern in self.exclusion_patterns:
                    if exclusion_pattern.match(title_text):
                        should_exclude = True
                        break
                
                if should_exclude:
                    continue
                
                if pattern_type == "å¯å‘å¼æ¨¡å¼":
                    start_line = text.rfind('\n', 0, match.start()) + 1
                    end_line = text.find('\n', match.end())
                    if end_line == -1: end_line = len(text)
                    
                    prev_line = text[text.rfind('\n', 0, start_line-2)+1:start_line-1].strip()
                    next_line = text[end_line+1:text.find('\n', end_line+1)].strip()

                    if not (prev_line == "" and next_line == ""):
                        continue 
                    
                    # å¯¹å¯å‘å¼æ¨¡å¼è¿›è¡Œé¢å¤–æ£€æŸ¥
                    # æ’é™¤çº¯æ•°å­—æˆ–è¿‡çŸ­çš„æ ‡é¢˜
                    if len(title_text) < 2 or title_text.isdigit():
                        continue
                    
                    # æ’é™¤åªåŒ…å«æ•°å­—å’Œæ ‡ç‚¹çš„æ ‡é¢˜
                    if re.match(r'^[\d\s\.\-_]+$', title_text):
                        continue

                is_duplicate = False
                for cand in candidates:
                    if cand.start_index == match.start():
                        is_duplicate = True
                        break
                if not is_duplicate:
                    candidates.append(PotentialChapter(
                        title_text=title_text,
                        start_index=match.start(),
                        end_index=match.end(),
                        confidence_score=score,
                        pattern_type=pattern_type
                    ))
        return sorted(candidates, key=lambda x: x.start_index)
        
    def _filter_and_merge_candidates(self, candidates: List[PotentialChapter]) -> List[PotentialChapter]:
        """
        è¿‡æ»¤ã€æ¶ˆæ­§å’Œåˆå¹¶å€™é€‰ç« èŠ‚ï¼Œè¿™æ˜¯ç®—æ³•çš„æ™ºèƒ½æ ¸å¿ƒã€‚
        """
        if not candidates:
            return []

        # æŒ‰ç½®ä¿¡åº¦é™åºæ’åºï¼Œä¼˜å…ˆä¿ç•™é«˜ç½®ä¿¡åº¦çš„ç« èŠ‚
        sorted_candidates = sorted(candidates, key=lambda x: (-x.confidence_score, x.start_index))
        
        final_candidates = []
        
        for current in sorted_candidates:
            should_add = True
            
            # æ£€æŸ¥ä¸å·²æ¥å—ç« èŠ‚çš„è·ç¦»
            for accepted in final_candidates:
                char_distance = abs(current.start_index - accepted.start_index)
                
                # åŠ¨æ€è°ƒæ•´æœ€å°è·ç¦»è¦æ±‚
                if current.confidence_score >= 80 and accepted.confidence_score >= 80:
                    # é«˜ç½®ä¿¡åº¦ç« èŠ‚ä¹‹é—´å…è®¸æ›´è¿‘çš„è·ç¦»
                    min_distance = 15  
                elif current.confidence_score >= 60 and accepted.confidence_score >= 60:
                    # ä¸­ç­‰ç½®ä¿¡åº¦ç« èŠ‚ä¹‹é—´çš„è·ç¦»
                    min_distance = 30
                else:
                    # ä½ç½®ä¿¡åº¦ç« èŠ‚éœ€è¦æ›´å¤§çš„è·ç¦»
                    min_distance = self.min_chapter_distance
                
                if char_distance < min_distance:
                    should_add = False
                    break
            
            if should_add:
                final_candidates.append(current)
        
        # æŒ‰ä½ç½®é‡æ–°æ’åº
        final_candidates.sort(key=lambda x: x.start_index)
        
        return final_candidates

    def _extract_content(self, text: str, chapters: List[PotentialChapter]) -> List[Chapter]:
        """
        æ ¹æ®æœ€ç»ˆçš„ç« èŠ‚æ ‡è®°åˆ—è¡¨ï¼Œåˆ‡åˆ†æ–‡æœ¬å¹¶æå–å†…å®¹ã€‚
        """
        if not chapters:
            return [Chapter(title="å…¨æ–‡", content=text.strip())]

        final_chapters = []
        
        first_chapter_start = chapters[0].start_index
        if first_chapter_start > 0:
            prologue_content = text[:first_chapter_start].strip()
            if prologue_content:
                final_chapters.append(Chapter(title="å‰è¨€", content=prologue_content))

        for i in range(len(chapters)):
            current_chap = chapters[i]
            
            if i + 1 < len(chapters):
                next_chap_start = chapters[i+1].start_index
            else:
                next_chap_start = len(text)
                
            content_start = current_chap.end_index
            content = text[content_start:next_chap_start].strip()

            final_chapters.append(Chapter(title=current_chap.title_text, content=content))
            
        return final_chapters

    def parse(self, text: str) -> List[Chapter]:
        """
        æ‰§è¡Œå®Œæ•´çš„è§£ææµç¨‹ã€‚
        :param text: å®Œæ•´çš„æ–‡ç« çº¯æ–‡æœ¬ã€‚
        :return: ä¸€ä¸ªåŒ…å«Chapterå¯¹è±¡çš„åˆ—è¡¨ã€‚
        """
        processed_text = self._preprocess(text)
        candidates = self._scan_for_candidates(processed_text)
        final_chapter_markers = self._filter_and_merge_candidates(candidates)
        result = self._extract_content(processed_text, final_chapter_markers)
        return result

# --- Bilingual UI Text & Configuration ---
UI_TEXT = {
    "en": {
        "title": "TXT to EPUB Converter", "upload_label": "1. Select TXT File", "add_file_button": "Add File to List", "file_list_header": "Selected Files", "clear_all_button": "Clear All", "cleaning_label": "2. Cleaning Options",
        "merge_lines": "Merge Empty Lines", "remove_spaces": "Remove Extra Spaces", "chapter_detection_header": "3. Chapter Detection",
        "detection_mode": "Mode", "intelligent_mode": "Intelligent Detection", "smart_chinese_mode": "Smart Chinese Parser", "custom_regex_mode": "Custom Regex",
        "custom_rule_label": "Custom Regex Rule", "custom_rule_info": "Used when 'Custom Regex' mode is selected.",
        "preview_button": "Preview Detected Chapters", "author_label": "4. Author (Optional)", "cover_label": "5. Cover Image (Optional)",
        "output_header": "6. Output Location", "output_path_label": "Output Folder Path (Optional)",
        "output_path_info": "If blank, files are saved to an 'epub_output' folder.", "save_to_source": "Save to Source File Location",
        "start_button": "Start Conversion",
        "chapter_preview_header": "Chapter Preview", "file_content_preview_header": "File Content Preview", "results_header": "Results", "log_header": "Log", "version": "Version 0.1.4",
        "lang_select": "Language / è¯­è¨€", "github_link": "ğŸ“¦ GitHub Repository: https://github.com/cs2764/txt-to-epub", "no_files_selected": "No files selected. Please add TXT files to the list.", "files_count": "files selected", "remove": "Remove",
    },
    "zh": {
        "title": "TXT è½¬ EPUB æ‰¹é‡è½¬æ¢å™¨", "upload_label": "1. é€‰æ‹© TXT æ–‡ä»¶", "add_file_button": "æ·»åŠ æ–‡ä»¶åˆ°åˆ—è¡¨", "file_list_header": "å·²é€‰æ‹©çš„æ–‡ä»¶", "clear_all_button": "æ¸…ç©ºæ‰€æœ‰", "cleaning_label": "2. æ¸…ç†é€‰é¡¹",
        "merge_lines": "åˆå¹¶ç©ºè¡Œ", "remove_spaces": "ç§»é™¤å¤šä½™ç©ºæ ¼", "chapter_detection_header": "3. ç« èŠ‚æ£€æµ‹",
        "detection_mode": "æ¨¡å¼", "intelligent_mode": "æ™ºèƒ½æ£€æµ‹", "smart_chinese_mode": "æ™ºèƒ½ä¸­æ–‡è§£æ", "custom_regex_mode": "è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼",
        "custom_rule_label": "è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™", "custom_rule_info": "å½“é€‰æ‹©\"è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼\"æ¨¡å¼æ—¶ä½¿ç”¨ã€‚",
        "preview_button": "é¢„è§ˆæ£€æµ‹åˆ°çš„ç« èŠ‚", "author_label": "4. ä½œè€…ï¼ˆå¯é€‰ï¼‰", "cover_label": "5. å°é¢å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰",
        "output_header": "6. è¾“å‡ºä½ç½®", "output_path_label": "è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
        "output_path_info": "å¦‚æœç•™ç©ºï¼Œæ–‡ä»¶å°†ä¿å­˜åˆ° 'epub_output' æ–‡ä»¶å¤¹ä¸­ã€‚", "save_to_source": "ä¿å­˜åˆ°æºæ–‡ä»¶ä½ç½®",
        "start_button": "å¼€å§‹è½¬æ¢",
        "chapter_preview_header": "ç« èŠ‚é¢„è§ˆ", "file_content_preview_header": "æ–‡ä»¶å†…å®¹é¢„è§ˆ", "results_header": "ç»“æœ", "log_header": "æ—¥å¿—", "version": "ç‰ˆæœ¬ 0.1.4",
        "lang_select": "Language / è¯­è¨€", "github_link": "ğŸ“¦ GitHub é¡¹ç›®åœ°å€ï¼šhttps://github.com/cs2764/txt-to-epub", "no_files_selected": "æœªé€‰æ‹©æ–‡ä»¶ã€‚è¯·æ·»åŠ  TXT æ–‡ä»¶åˆ°åˆ—è¡¨ä¸­ã€‚", "files_count": "ä¸ªæ–‡ä»¶å·²é€‰æ‹©", "remove": "åˆ é™¤",
    }
}

# --- Core Engine (No changes from previous version) ---
HEURISTIC_PATTERNS = [
    (re.compile(r"^\s*ç¬¬\s*[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+\s*[ç« ç« èŠ‚å›]"), 30), (re.compile(r"^\s*å·\s*[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+"), 28),
    (re.compile(r"^\s*chapter\s*\d+", re.IGNORECASE), 25), (re.compile(r"^\s*#+"), 15),
    (re.compile(r"^\s*ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+\ï¼‰.*"), 20), # New pattern for (ä¸€)Chapter Title
]
DISQUALIFYING_PATTERNS = [re.compile(r"[ã€‚ï¼Ÿï¼""''(ï¼ˆ)ï¼‰â€”]"), re.compile(r'^("|\')'), re.compile(r"(\w{3,}\s){5,}")]

def clean_text(text, options, lang):
    if UI_TEXT[lang]["merge_lines"] in options:
        text = re.sub(r'\n\s*\n', '\n', text)
    if UI_TEXT[lang]["remove_spaces"] in options:
        text = '\n'.join(line.strip() for line in text.split('\n'))
    return text

def is_valid_chapter_candidate(line, text_len, line_pos):
    for pattern in DISQUALIFYING_PATTERNS:
        if pattern.search(line): return False
    if line_pos / text_len > 0.95: return False
    for pattern, _ in HEURISTIC_PATTERNS:
        if pattern.search(line): return True
    return False

def intelligent_chapter_detection(text):
    lines = text.split('\n')
    text_len = len(text)
    if text_len == 0: return [], None
    chapters = []
    line_positions = [m.start() for m in re.finditer(r'^.*$', text, re.MULTILINE)]
    for i, line in enumerate(lines):
        line = line.strip()
        if not line: continue
        line_pos = line_positions[i]
        if is_valid_chapter_candidate(line, text_len, line_pos):
             is_preceded = (i == 0 or not lines[i-1].strip())
             is_followed = (i == len(lines) - 1 or not lines[i+1].strip())
             if is_preceded and is_followed:
                 chapters.append((line, line_pos))
    if not chapters:
        simple_pattern = re.compile(r"^\s*(chapter\s*\d+|ç¬¬\s*\d+\s*ç« )", re.IGNORECASE | re.MULTILINE)
        chapters = [(m.group(0).strip(), m.start()) for m in simple_pattern.finditer(text)]
    return chapters, None

def smart_chinese_chapter_detection(text):
    """
    ä½¿ç”¨æ™ºèƒ½ä¸­æ–‡ç« èŠ‚è§£æå™¨è¿›è¡Œç« èŠ‚æ£€æµ‹
    """
    try:
        parser = SmartChapterParser()
        chapters = parser.parse(text)
        
        # è½¬æ¢ä¸ºç°æœ‰ç¨‹åºæœŸæœ›çš„æ ¼å¼ (title, position)
        result_chapters = []
        
        # é‡æ–°è§£æåŸæ–‡ä»¥è·å–å‡†ç¡®çš„ç« èŠ‚ä½ç½®
        processed_text = parser._preprocess(text)
        candidates = parser._scan_for_candidates(processed_text)
        final_chapter_markers = parser._filter_and_merge_candidates(candidates)
        
        # å¦‚æœæœ‰å‰è¨€ï¼Œå…ˆå¤„ç†å‰è¨€
        if chapters and chapters[0].title == "å‰è¨€":
            result_chapters.append((chapters[0].title, 0))
            # ä»ç¬¬äºŒä¸ªç« èŠ‚å¼€å§‹å¤„ç†
            start_idx = 1
        else:
            start_idx = 0
        
        # å¤„ç†å®é™…çš„ç« èŠ‚æ ‡è®°
        for i, marker in enumerate(final_chapter_markers):
            chapter_idx = start_idx + i
            if chapter_idx < len(chapters):
                result_chapters.append((chapters[chapter_idx].title, marker.start_index))
        
        return result_chapters, None
    except Exception as e:
        return [], f"Smart Chinese parser error: {str(e)}"

def detect_chapters_from_custom_pattern(text, pattern_str):
    if not pattern_str: return [], "Custom pattern cannot be empty."
    try:
        pattern = re.compile(pattern_str, re.MULTILINE)
        return [(match.group(0).strip(), match.start()) for match in pattern.finditer(text)], None
    except re.error:
        return [], "Invalid Custom Regex Pattern."

def convert_to_epub(text, filename, author, cover_image_path, chapters):
    book = epub.EpubBook()
    book.set_identifier(os.path.basename(filename))
    book.set_title(os.path.splitext(os.path.basename(filename))[0])
    book.set_language('zh')
    if author: book.add_author(author)
    if cover_image_path and os.path.exists(cover_image_path):
        book.set_cover("cover.jpg", open(cover_image_path, 'rb').read())
    if not chapters: chapters = [("Content", 0)]
    book.toc = []
    book.spine = ['nav']
    for i, (title, start) in enumerate(chapters):
        end = chapters[i + 1][1] if i + 1 < len(chapters) else len(text)
        content_html = text[start:end].replace('\n', '<br/>')
        chapter_filename = f'chap_{i+1}.xhtml'
        c = epub.EpubHtml(title=title, file_name=chapter_filename, lang='zh_cn')
        c.content = f'<h1>{title}</h1>{content_html}'
        book.add_item(c)
        book.toc.append(epub.Link(chapter_filename, title, f'chap_{i+1}'))
        book.spine.append(c)
    book.add_item(epub.EpubNcx()); book.add_item(epub.EpubNav())
    return book

def process_files_and_convert(files, clean_options, detection_mode, custom_rule, author, cover_image, save_to_source, custom_path, lang_key):
    if not files: 
        error_msg = "Please add at least one TXT file to the list." if lang_key == "en" else "è¯·è‡³å°‘æ·»åŠ ä¸€ä¸ª TXT æ–‡ä»¶åˆ°åˆ—è¡¨ä¸­ã€‚"
        return [], error_msg, ""
    
    # Default output directory
    default_output_dir = custom_path if custom_path and os.path.isdir(custom_path) else os.path.join(os.getcwd(), "epub_output")
    
    logs = ""; results_data = []
    for file_obj in tqdm(files, desc="Converting files"):
        original_filename = os.path.basename(file_obj.name)
        
        # Determine output directory for this file
        if save_to_source:
            # Try to save to source file location
            source_dir = os.path.dirname(file_obj.name)
            try:
                # Test if we can write to the source directory
                test_path = os.path.join(source_dir, ".test_write_permission")
                with open(test_path, 'w') as test_file:
                    test_file.write("test")
                os.remove(test_path)
                output_dir = source_dir
                logs += f"Saving {original_filename} to source location: {source_dir}\n"
            except (OSError, PermissionError, IOError) as e:
                # Fallback to default directory if can't write to source
                output_dir = default_output_dir
                os.makedirs(output_dir, exist_ok=True)
                logs += f"Cannot write to source location for {original_filename} ({e}), using default: {output_dir}\n"
        else:
            output_dir = default_output_dir
            os.makedirs(output_dir, exist_ok=True)
            if not logs.startswith("Output directory:"):
                logs += f"Output directory: {output_dir}\n"
        
        try:
            # Enhanced encoding detection
            encoding, confidence, detection_method = detect_file_encoding(file_obj.name)
            
            logs += f"Detected encoding for {original_filename}: {encoding} (confidence: {confidence:.2f}, method: {detection_method})\n"

            with open(file_obj.name, 'r', encoding=encoding, errors='replace') as f:
                text = f.read()
            if clean_options: text = clean_text(text, clean_options, lang_key)
            if detection_mode == UI_TEXT[lang_key]["intelligent_mode"]:
                chapters, error_msg = intelligent_chapter_detection(text)
            elif detection_mode == UI_TEXT[lang_key]["smart_chinese_mode"]:
                chapters, error_msg = smart_chinese_chapter_detection(text)
            else:
                chapters, error_msg = detect_chapters_from_custom_pattern(text, custom_rule)
            if error_msg: logs += f"Error for {original_filename}: {error_msg}\n"
            epub_book = convert_to_epub(text, original_filename, author, cover_image, chapters)
            output_path = os.path.join(output_dir, f"{os.path.splitext(original_filename)[0]}.epub")
            epub.write_epub(output_path, epub_book)
            results_data.append([original_filename, output_path, "Success"])
            logs += f"Successfully converted {original_filename} to {output_path}\n"
        except Exception as e:
            logs += f"Failed to convert {original_filename}: {e}\n"
            results_data.append([original_filename, "", f"Failed: {e}"])
    return results_data, logs, ""

def preview_chapters(files, detection_mode, custom_rule, lang_key):
    if not files: 
        return "Upload a file to preview chapters." if lang_key == "en" else "è¯·æ·»åŠ æ–‡ä»¶ä»¥é¢„è§ˆç« èŠ‚ã€‚"
    try:
        # Handle file list properly - use first file for preview
        file_obj = files[0] if isinstance(files, list) and len(files) > 0 else files
        if not file_obj:
            return "No files in the list." if lang_key == "en" else "åˆ—è¡¨ä¸­æ²¡æœ‰æ–‡ä»¶ã€‚"
        file_path = file_obj.name if hasattr(file_obj, 'name') else str(file_obj)
        
        # Enhanced encoding detection
        encoding, confidence, detection_method = detect_file_encoding(file_path)

        preview_log = f"Detected encoding for {os.path.basename(file_path)}: {encoding} (confidence: {confidence:.2f}, method: {detection_method})\n"

        with open(file_path, 'r', encoding=encoding, errors='replace') as f:
            text = f.read()
        
        if detection_mode == UI_TEXT[lang_key]["intelligent_mode"]:
            chapters, error_msg = intelligent_chapter_detection(text)
        elif detection_mode == UI_TEXT[lang_key]["smart_chinese_mode"]:
            chapters, error_msg = smart_chinese_chapter_detection(text)
        else:
            chapters, error_msg = detect_chapters_from_custom_pattern(text, custom_rule)
        
        if error_msg: return f"Error: {error_msg}"
        if chapters: 
            chapter_list = "\n".join([f"- {c[0]}" for c in chapters])
            return f"Detected Chapters ({len(chapters)} found):\n{chapter_list}"
        return "No chapters detected."
    except Exception as e:
        return f"Error reading file: {str(e)}"

# --- Enhanced Encoding Detection Function ---
def detect_file_encoding(file_path):
    """
    å¢å¼ºçš„æ–‡ä»¶ç¼–ç æ£€æµ‹å‡½æ•°ï¼Œæ”¯æŒå¤šç§æ£€æµ‹æ–¹æ³•
    """
    # Common encodings to try in order
    common_encodings = [
        'utf-8-sig',  # UTF-8 with BOM
        'utf-8',      # UTF-8
        'gbk',        # Chinese GBK
        'gb2312',     # Chinese GB2312
        'cp936',      # Chinese CP936 (similar to GBK)
        'big5',       # Traditional Chinese Big5
        'ascii',      # ASCII
        'utf-16',     # UTF-16
        'utf-32',     # UTF-32
    ]
    
    try:
        with open(file_path, 'rb') as f:
            # Read more data for better detection (up to 64KB)
            raw_data = f.read(65536)
        
        # Check for BOM first
        if raw_data.startswith(b'\xef\xbb\xbf'):
            return 'utf-8-sig', 1.0, 'BOM detected'
        elif raw_data.startswith(b'\xff\xfe'):
            return 'utf-16-le', 1.0, 'BOM detected'
        elif raw_data.startswith(b'\xfe\xff'):
            return 'utf-16-be', 1.0, 'BOM detected'
        elif raw_data.startswith(b'\xff\xfe\x00\x00'):
            return 'utf-32-le', 1.0, 'BOM detected'
        elif raw_data.startswith(b'\x00\x00\xfe\xff'):
            return 'utf-32-be', 1.0, 'BOM detected'
        
        # Use chardet for detection
        detection = chardet.detect(raw_data)
        detected_encoding = detection.get('encoding', '').lower()
        confidence = detection.get('confidence', 0.0)
        
        # If confidence is high enough, use detected encoding
        if confidence > 0.7 and detected_encoding:
            return detected_encoding, confidence, 'chardet'
        
        # Try common encodings and pick the one that works best
        best_encoding = None
        best_score = 0
        best_method = 'fallback'
        
        for encoding in common_encodings:
            try:
                decoded_text = raw_data.decode(encoding)
                # Score based on Chinese characters and readable content
                chinese_chars = sum(1 for c in decoded_text if '\u4e00' <= c <= '\u9fff')
                total_chars = len(decoded_text)
                
                if total_chars > 0:
                    chinese_ratio = chinese_chars / total_chars
                    # Higher score for files with Chinese content
                    score = chinese_ratio * 2 + 0.5  # Base score
                    
                    if score > best_score:
                        best_score = score
                        best_encoding = encoding
                        best_method = 'pattern_match'
                        
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        # If we found a good encoding, use it
        if best_encoding:
            return best_encoding, best_score, best_method
        
        # Last resort: use chardet result or utf-8
        return detected_encoding or 'utf-8', confidence, 'last_resort'
        
    except Exception as e:
        return 'utf-8', 0.0, f'error: {str(e)}'

# --- File Management Functions ---
def add_file_to_list(new_file, current_files, lang_key):
    """
    æ·»åŠ æ–‡ä»¶åˆ°æ–‡ä»¶åˆ—è¡¨
    """
    if not new_file:
        return current_files, format_file_list(current_files, lang_key), update_file_preview_for_list(current_files, lang_key)
    
    # åˆå§‹åŒ–æ–‡ä»¶åˆ—è¡¨
    if not current_files:
        current_files = []
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    new_file_path = new_file.name if hasattr(new_file, 'name') else str(new_file)
    for existing_file in current_files:
        existing_path = existing_file.name if hasattr(existing_file, 'name') else str(existing_file)
        if os.path.basename(new_file_path) == os.path.basename(existing_path):
            # æ–‡ä»¶å·²å­˜åœ¨ï¼Œä¸é‡å¤æ·»åŠ 
            return current_files, format_file_list(current_files, lang_key), update_file_preview_for_list(current_files, lang_key)
    
    # æ·»åŠ æ–°æ–‡ä»¶
    current_files.append(new_file)
    return current_files, format_file_list(current_files, lang_key), update_file_preview_for_list(current_files, lang_key)

def remove_file_from_list(file_index, current_files, lang_key):
    """
    ä»æ–‡ä»¶åˆ—è¡¨ä¸­åˆ é™¤æŒ‡å®šçš„æ–‡ä»¶
    """
    if not current_files or file_index < 0 or file_index >= len(current_files):
        return current_files, format_file_list(current_files, lang_key)
    
    current_files.pop(file_index)
    return current_files, format_file_list(current_files, lang_key)

def clear_all_files(lang_key):
    """
    æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶
    """
    return [], format_file_list([], lang_key), update_file_preview_for_list([], lang_key)

def format_file_list(files, lang_key):
    """
    æ ¼å¼åŒ–æ–‡ä»¶åˆ—è¡¨æ˜¾ç¤º
    """
    if not files:
        return UI_TEXT[lang_key]["no_files_selected"]
    
    file_count = len(files)
    if lang_key == "en":
        header = f"**{file_count} {UI_TEXT[lang_key]['files_count']}**\n\n"
    else:
        header = f"**{file_count} {UI_TEXT[lang_key]['files_count']}**\n\n"
    
    file_lines = []
    for i, file_obj in enumerate(files):
        filename = os.path.basename(file_obj.name) if hasattr(file_obj, 'name') else str(file_obj)
        file_size = os.path.getsize(file_obj.name) if hasattr(file_obj, 'name') and os.path.exists(file_obj.name) else 0
        size_str = f"{file_size:,} bytes" if lang_key == "en" else f"{file_size:,} å­—èŠ‚"
        file_lines.append(f"{i+1}. **{filename}** ({size_str})")
    
    return header + "\n".join(file_lines)

def update_file_preview_for_list(files, lang_key):
    """
    ä¸ºæ–‡ä»¶åˆ—è¡¨æ›´æ–°é¢„è§ˆå†…å®¹ï¼ˆæ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡ä»¶çš„é¢„è§ˆï¼‰
    """
    if not files:
        return UI_TEXT[lang_key]["no_files_selected"]
    
    return preview_file_content(files, lang_key)

# --- NEW: File Content Preview Function ---
def preview_file_content(files, lang_key):
    """
    é¢„è§ˆæ–‡ä»¶çš„å‰10è¡Œå†…å®¹ï¼Œä½¿ç”¨å¢å¼ºçš„ç¼–ç æ£€æµ‹
    """
    if not files: 
        return "Please upload a file to preview content." if lang_key == "en" else "è¯·ä¸Šä¼ æ–‡ä»¶ä»¥é¢„è§ˆå†…å®¹ã€‚"
    
    try:
        # Handle file object properly
        file_obj = files[0] if isinstance(files, list) else files
        file_path = file_obj.name if hasattr(file_obj, 'name') else str(file_obj)
        
        # Enhanced encoding detection
        encoding, confidence, detection_method = detect_file_encoding(file_path)
        
        # Read and preview file content with detected encoding
        lines = []
        file_size = os.path.getsize(file_path)
        filename = os.path.basename(file_path)
        
        try:
            with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                for i, line in enumerate(f):
                    if i >= 10:  # Only read first 10 lines
                        break
                    lines.append(line.rstrip())
        except Exception as read_error:
            # Fallback to utf-8 with error replacement
            encoding = 'utf-8'
            detection_method = 'fallback_utf8'
            confidence = 0.0
            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                for i, line in enumerate(f):
                    if i >= 10:
                        break
                    lines.append(line.rstrip())
        
        # Create detailed preview header
        if lang_key == "en":
            preview_header = (
                f"**File**: {filename}\n"
                f"**Size**: {file_size:,} bytes\n"
                f"**Encoding**: {encoding} (confidence: {confidence:.2f}, method: {detection_method})\n"
                f"\n**Preview (First 10 lines):**\n"
            )
            more_content = f"\n\n*(File contains more content beyond these 10 lines)*" if len(lines) == 10 else ""
        else:
            preview_header = (
                f"**æ–‡ä»¶**: {filename}\n"
                f"**å¤§å°**: {file_size:,} å­—èŠ‚\n"
                f"**ç¼–ç **: {encoding} (ç½®ä¿¡åº¦: {confidence:.2f}, æ£€æµ‹æ–¹æ³•: {detection_method})\n"
                f"\n**é¢„è§ˆ (å‰10è¡Œ):**\n"
            )
            more_content = f"\n\n*(æ–‡ä»¶åŒ…å«æ›´å¤šå†…å®¹ï¼Œè¶…å‡ºè¿™10è¡Œ)*" if len(lines) == 10 else ""
        
        # Format the preview with line numbers and handle empty lines
        preview_lines = []
        for i, line in enumerate(lines, 1):
            display_line = line if line.strip() else '(ç©ºè¡Œ)'
            preview_lines.append(f"{i:2d}: {display_line}")
        
        # Add warning for low confidence
        warning = ""
        if confidence < 0.5 and lang_key == "zh":
            warning = "\nâš ï¸ **æ³¨æ„**: ç¼–ç æ£€æµ‹ç½®ä¿¡åº¦è¾ƒä½ï¼Œå¦‚æœæ˜¾ç¤ºä¹±ç ï¼Œæ–‡ä»¶å¯èƒ½ä½¿ç”¨äº†å…¶ä»–ç¼–ç æ ¼å¼ã€‚"
        elif confidence < 0.5 and lang_key == "en":
            warning = "\nâš ï¸ **Warning**: Low encoding detection confidence. If text appears garbled, the file may use a different encoding."
        
        preview_content = preview_header + "\n".join(preview_lines) + more_content + warning
        
        return preview_content
        
    except Exception as e:
        error_msg = f"Error reading file: {str(e)}" if lang_key == "en" else f"è¯»å–æ–‡ä»¶é”™è¯¯: {str(e)}"
        return error_msg

# --- UI Creation Function ---
def create_ui(lang_key):
    LANG = UI_TEXT[lang_key]
    with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue", secondary_hue="sky")) as demo:
        # Language switcher is now a state component
        lang_state = gr.State(value=lang_key)
        # File list state to store selected files
        file_list_state = gr.State(value=[])

        with gr.Row():
            gr.Markdown(f"# {LANG['title']}")
            lang_radio = gr.Radio(["en", "zh"], label=LANG["lang_select"], value=lang_key, interactive=True)
        
        with gr.Row():
            with gr.Column(scale=2):
                # File selection section
                file_input = gr.File(label=LANG["upload_label"], file_count="single", file_types=[".txt"])
                with gr.Row():
                    add_file_button = gr.Button(LANG["add_file_button"], variant="secondary")
                    clear_all_button = gr.Button(LANG["clear_all_button"], variant="secondary")
                
                # File list display
                file_list_display = gr.Markdown(LANG["no_files_selected"], label=LANG["file_list_header"])
                
                cleaning_options = gr.CheckboxGroup([LANG["merge_lines"], LANG["remove_spaces"]], label=LANG["cleaning_label"], value=[LANG["merge_lines"], LANG["remove_spaces"]])
                gr.Markdown(f"### {LANG['chapter_detection_header']}")
                detection_mode = gr.Radio([LANG["intelligent_mode"], LANG["smart_chinese_mode"], LANG["custom_regex_mode"]], label=LANG["detection_mode"], value=LANG["intelligent_mode"])
                custom_rule_input = gr.Textbox(label=LANG["custom_rule_label"], info=LANG["custom_rule_info"], visible=False)
                preview_button = gr.Button(LANG["preview_button"])
                author_input = gr.Textbox(label=LANG["author_label"])
                cover_image_input = gr.Image(label=LANG["cover_label"], type="filepath")
                gr.Markdown(f"### {LANG['output_header']}")
                save_to_source_checkbox = gr.Checkbox(label=LANG["save_to_source"], value=False)
                custom_path_input = gr.Textbox(label=LANG["output_path_label"], info=LANG["output_path_info"])
                start_button = gr.Button(LANG["start_button"], variant="primary")
            with gr.Column(scale=3):
                # NEW: File Content Preview
                file_content_preview_output = gr.Markdown(label=LANG["file_content_preview_header"])
                chapter_preview_output = gr.Markdown(label=LANG["chapter_preview_header"])
                results_output = gr.Dataframe(headers=["Source File", "Output Path", "Status"], label=LANG["results_header"], row_count=10)
                log_output = gr.Textbox(label=LANG["log_header"], lines=15, interactive=False)
        gr.Markdown("---")
        version_info = gr.Markdown(LANG["version"])
        github_info = gr.Markdown(LANG["github_link"])

        # --- Event Handlers ---
        detection_mode.change(lambda x: gr.update(visible=x == LANG["custom_regex_mode"]), detection_mode, custom_rule_input)
        
        # File management event handlers
        add_file_button.click(
            fn=add_file_to_list,
            inputs=[file_input, file_list_state, lang_state],
            outputs=[file_list_state, file_list_display, file_content_preview_output]
        )
        
        clear_all_button.click(
            fn=clear_all_files,
            inputs=[lang_state],
            outputs=[file_list_state, file_list_display, file_content_preview_output]
        )
        
        # Note: Removed file_list_state.change() as it's not supported in Gradio 3.50.2
        # File preview will be updated through add_file and clear_all functions
        
        start_button.click(
            fn=process_files_and_convert,
            inputs=[file_list_state, cleaning_options, detection_mode, custom_rule_input, author_input, cover_image_input, save_to_source_checkbox, custom_path_input, lang_state],
            outputs=[results_output, log_output, chapter_preview_output]
        )
        preview_button.click(
            fn=preview_chapters,
            inputs=[file_list_state, detection_mode, custom_rule_input, lang_state],
            outputs=chapter_preview_output
        )
        
        # The magic for language switching: re-render the UI on change
        # This is a bit of a hack, but it's the standard way in Gradio
        # It works by updating every single component with its new label from the language dict
        components_to_update = [
            file_input, add_file_button, clear_all_button, file_list_display, cleaning_options, detection_mode, custom_rule_input, preview_button,
            author_input, cover_image_input, save_to_source_checkbox, custom_path_input, start_button,
            file_content_preview_output, chapter_preview_output, results_output, log_output, version_info, github_info
        ]
        
        def update_lang_func(lang_key_new):
            NEW_LANG = UI_TEXT[lang_key_new]
            return (
                lang_key_new, # Update the state
                gr.update(label=NEW_LANG["upload_label"]),
                gr.update(value=NEW_LANG["add_file_button"]),
                gr.update(value=NEW_LANG["clear_all_button"]),
                gr.update(label=NEW_LANG["file_list_header"], value=NEW_LANG["no_files_selected"]),
                gr.update(label=NEW_LANG["cleaning_label"], choices=[NEW_LANG["merge_lines"], NEW_LANG["remove_spaces"]], value=[NEW_LANG["merge_lines"], NEW_LANG["remove_spaces"]]),
                gr.update(label=NEW_LANG["detection_mode"], choices=[NEW_LANG["intelligent_mode"], NEW_LANG["smart_chinese_mode"], NEW_LANG["custom_regex_mode"]]),
                gr.update(label=NEW_LANG["custom_rule_label"], info=NEW_LANG["custom_rule_info"]),
                gr.update(value=NEW_LANG["preview_button"]),
                gr.update(label=NEW_LANG["author_label"]),
                gr.update(label=NEW_LANG["cover_label"]),
                gr.update(label=NEW_LANG["save_to_source"]),
                gr.update(label=NEW_LANG["output_path_label"], info=NEW_LANG["output_path_info"]),
                gr.update(value=NEW_LANG["start_button"]),
                gr.update(label=NEW_LANG["file_content_preview_header"]),
                gr.update(label=NEW_LANG["chapter_preview_header"]),
                gr.update(label=NEW_LANG["results_header"]),
                gr.update(label=NEW_LANG["log_header"]),
                gr.update(value=NEW_LANG["version"]),
                gr.update(value=NEW_LANG["github_link"]),
            )

        lang_radio.change(
            fn=update_lang_func,
            inputs=lang_radio,
            outputs=[lang_state] + components_to_update
        )
    return demo

if __name__ == "__main__":
    app = create_ui("zh") # Start with Chinese as default
    print("ğŸš€ Starting TXT to EPUB Converter...")
    print("ğŸ“ Local access: http://localhost:7860")
    print("ğŸŒ Network access: http://0.0.0.0:7860")
    print("âš ï¸  Using local server for better stability")
    app.launch(inbrowser=True, server_name="0.0.0.0", share=False)